{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1G0Bq5gNcG2nD9YMS8O8u0HdylunKmctw","timestamp":1711335668487},{"file_id":"1tblKSOB_eMJqZ8uOi_mW243ZiLHvUkq6","timestamp":1711222373013}],"authorship_tag":"ABX9TyN5lCF/MB0wWwXjjVwGE0aw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Homework 7\n","\n","Adaptation of Question 5.1 from Loucks and van Beek / question 2 from HW 5. A reservoir serves multiple purposes. It is a recreation site for swimmers, wind surfers and boaters; a flood storage reservoir; and an irrigation supply structure. Each season, there is a required minimum release for irrigation to meet water rights downstream. Reservoir releases in excess of the irrigation requirement flow to a wetland downstream. The wetland also receives return flows from irrigation, equal to 30\\% of the water diverted for irrigation. These return flows are highly saline, which can damage the ecosystem. Different target water allocations are favorable for each of these objectives. Based on mean inflows across 4 seasons, your task is to determine how much water should be released from the reservoir in each season to minimize the sum of percent deviation across all targets given the following parameters and targets:\n","\n","* Reservoir storage capacity, $K$: 30 million m$^3$ (mcm)\n","* Maximum reservoir release, $Rmax$: 50 mcm\n","* Reservoir seasonal inflows distribution, $Y=\\ln(Qin)$: $N(1.8, 0.1)$, $N(3.6, 0.3)$, $N(2.9, 0.3)$, $N(2.3, 0.2)$ ($Qin$ in mcm)\n","* Correlation between consecutive season's log-space flows: $\\rho=0.7$\n","* Minimum releases for irrigation each season, $Qirr$: 5, 20, 10, 5 mcm\n","* Salinity concentration of reservoir water, $Cres$: 1 part per trillion (ppt)\n","* Salinity concentration of irrigation return flow water, $Cirr$: 20 ppt\n","* Target salinity concentration in the wetland, $Cwet\\_maxTarget$: 3 ppt\n","* Target minimum flow in the wetland each season, $Qwet\\_minTarget$: 10, 20, 15, 15 mcm\n","* Target reservoir storages each period, $S$: 20, 5, 20, 20 mcm"],"metadata":{"id":"qCBpkztdnCsO"}},{"cell_type":"markdown","source":["## Part a\n","\n","Re-run your dynamic program from HW 5 to find the optimal releases from the reservoir each season for discrete storage states between 0 and 30 in increments of 5, assuming the mean inflow each season. Use the same cost function as HW5: the sum of percent deviations from all targets, where storages above and below the target are penalized but only wetland salinity concentrations below the minimum target are penalized, and only wetland flows above the maximum target. Report a table with the prescribed release each season from each storage state."],"metadata":{"id":"tyebJImDnnh-"}},{"cell_type":"markdown","source":["$\\color{red}{\\text{Complete the code below with the inflow parameters}}$"],"metadata":{"id":"rDqZe5B-j2OH"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import scipy.stats as ss\n","import matplotlib.pyplot as plt\n","\n","# inflow parameters\n","muY =\n","sigmaY =\n","nSeasons = len(muY)\n","meanQ =\n","rho ="],"metadata":{"id":"2KSVshvkkAEG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["$\\color{red}{\\text{The code below is complete - run it to get the optimal DDP policy.}}$"],"metadata":{"id":"eEvIn_SIkAxi"}},{"cell_type":"code","source":["# define model parameters\n","# reservoir parameters\n","K = 30\n","maxR = 50\n","S_target = np.array([20, 5, 20, 20])\n","\n","# irrigation parameters\n","Qirr = np.array([5,20,10,5])\n","Qirr_RF =  0.3\n","\n","# salinity parameters\n","Cres = 1\n","Cirr = 20\n","Cwet_maxTarget = 5\n","\n","# wetland parameters\n","Qwet_minTarget = np.array([10,20,15,15])\n","\n","######################## DP Optimization ########################\n","\n","def calcCostDP(S, Q, Qirr, S_target, Cwet_maxTarget, Qwet_minTarget, bounds, FutureCost):\n","  '''\n","  Function to calculate the optimal release (Rbest) from each storage state (S)\n","  and associated present and future cost (Cbest), defined as the total squared\n","  deviation between storage and release targets for that stage.\n","\n","  Inputs:\n","    S: 1-D array of discrete storage values representing the states\n","    Q: inflow received at this stage (scalar)\n","    Qirr: irrigation flow this stage (scalar)\n","    S_target: target storage for next stage (scalar)\n","    Cwet_target: target maximum salinity in the wetland for this stage (scalar)\n","    Qwet_minTarget: target minimum flow in the wetland for this stage (scalar)\n","    bounds: bounds on possible releases (1-D array of length 2)\n","    FutureCost: 1-D array of future costs at each state that will be added to\n","      cost of the optimal state transition at this stage to compute present + future cost\n","\n","  Outputs:\n","    Rbest: 1-D array of optimal releases from each state in S\n","    Cbest: 1-D array of present + future costs associated with each state S\n","  '''\n","\n","  # initialize current cost at infinity and releases at 0\n","  Cbest = np.empty([len(S)])+np.inf\n","  Rbest = np.zeros([len(S)])\n","  for i, s in enumerate(S): # storage at stage t\n","    # find optimal storage to move to at stage t+1\n","    for j, sNext in enumerate(S): # storage at stage t+1\n","      R = s + Q - sNext # release to get to sNext\n","      # find cost of this release if it's feasible\n","      if R >= bounds[0] and R <= bounds[1]:\n","        # compute flow and salinity in the wetland\n","        Qwet = R - Qirr + Qirr_RF*Qirr\n","        Cwet = (Cres*(R - Qirr) + Cirr*Qirr_RF*Qirr) / Qwet\n","\n","        # compute total cost C (total deviation from targets + future cost at sNext)\n","        S_deviation = np.abs(sNext-S_target)*100/S_target\n","        Qwet_deviation = max(Qwet_minTarget - Qwet,0)*100 / Qwet_minTarget\n","        Cwet_deviation = max(Cwet - Cwet_maxTarget,0)*100 / Cwet_maxTarget\n","        C = S_deviation + Qwet_deviation + Cwet_deviation + FutureCost[j]\n","        # update optimal value (Cbest) and decision (Rbest) if better than current best\n","        if C < Cbest[i]:\n","          Cbest[i] = C\n","          Rbest[i] = R\n","\n","  return Rbest, Cbest\n","\n","# get indices of stages\n","nStages = len(meanQ)\n","forward_indices = np.arange(nStages)\n","backward_indices = forward_indices[::-1]\n","backward_indices = np.insert(backward_indices,0,0)\n","\n","# discretize states\n","states = np.arange(0,31,5)\n","nStates = len(states)\n","\n","# bounds on decision variables (releases)\n","# R is between Qirr and 50, S is positive and can't exceed capacity K\n","bounds = []\n","for i in range(len(Qirr)):\n","    bounds.append([Qirr[i],maxR])\n","\n","# initialize matrices with costs of each state at each stage\n","# and optimal releases to make from each state at each stage\n","DDP_costs = np.empty([nStates,nStages])\n","DDP_release_policy = np.empty([nStates,nStages])\n","\n","# initialize FutureCost at 0 for all states; will update as we move backwards\n","FutureCost = np.zeros([nStates])\n","\n","# begin backward-moving DP\n","loop = True\n","while loop:\n","  count = 0\n","  for index in backward_indices[0:-1]:\n","    # find optimal release and value of each state in this stage\n","    R, FutureCost = calcCostDP(states, meanQ[index-1], Qirr[index-1], S_target[index], Cwet_maxTarget,\n","                               Qwet_minTarget[index-1], bounds[index-1], FutureCost)\n","\n","    # count iterations with no change in optimal release\n","    if np.all(R == DDP_release_policy[:,index-1]):\n","      count += 1\n","\n","    # update best releases and value of each state if not yet in steady state\n","    DDP_costs[:,index] = FutureCost\n","    DDP_release_policy[:,index-1] = R\n","\n","  # stop loop if no change in optimal decisions across all iterations\n","  if count == len(backward_indices[0:-1]):\n","    break\n","\n","release_policy_df = pd.DataFrame(DDP_release_policy,\n","                                 columns=[\"Season 1\",\"Season 2\",\"Season 3\",\"Season 4\"],\n","                                 index=states)\n","release_policy_df.index.rename(\"Storage\",inplace=True)\n","release_policy_df"],"metadata":{"id":"TXQNnv_dBOff"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part b\n","\n","Now explicitly consider uncertainty in the optimization using stochastic dynamic programming. First, compute the transition probabilities from 15 discrete log-space flow levels $Y_{t-1}$ in season $t-1$ to 15 discrete log-space flow levels $Y_t$ in season $t$. Print the transition probabilities each season."],"metadata":{"id":"7Y9Icg6ooPIF"}},{"cell_type":"markdown","source":["$\\color{red}{\\text{Complete the code below to calculate and print the transition probabilities}}$"],"metadata":{"id":"KmihRTVckSNJ"}},{"cell_type":"code","source":[],"metadata":{"id":"yVqo9vLLTtu_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part c\n","\n","Modify the functions $\\texttt{calcCostSDP}$ and $\\texttt{findBestR}$ from the [SDPexample.ipynb](https://colab.research.google.com/github/EnvSystemsUVA/CodingExamples/blob/main/10_SDPexample.ipynb) notebook shown in class to calculate the sum of present and expected future costs, where costs are quantified the same as in part (a). Then optimize the SDP policy, iterating through the years until the average percent difference in the release policy is $<$ 0.1\\% across all seasons. Constrain the SDP policy to not violate the storage constraints under the mean inflow. Print the release policy as a function of the storage states and flow levels each season."],"metadata":{"id":"1xUZoD_0hqd6"}},{"cell_type":"markdown","source":["$\\color{red}{\\text{Complete the functions below to calculate present and expected future costs.}}$"],"metadata":{"id":"j49wrPEZkbbi"}},{"cell_type":"code","source":["######################## SDP Optimization ########################\n","from scipy.optimize import minimize\n","\n","def findBestR(R, s, y1, j, S, Ylevels2, transprob, Qirr, S_target, Cwet_maxTarget, Qwet_minTarget, FutureExpCost):\n","\n","\n","  return C\n","\n","def calcCostSDP(S, Ylevels1, Ylevels2, Qguess, transprob, Qirr, S_target, Cwet_maxTarget, Qwet_minTarget, bounds, FutureExpCost):\n","\n","\n","  return Rbest, Cbest"],"metadata":{"id":"DZ-3m-C2UJeG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["$\\color{red}{\\text{Complete the code below to find the optimal release policy with SDP}}$"],"metadata":{"id":"PaVUJ4KFk6gW"}},{"cell_type":"code","source":["# use same stages, states, indices and bounds as for DDP\n","# initialize matrices with costs of each state at each stage\n","# and optimal releases to make from each state at each stage\n","SDP_costs = np.empty([nStates, nLevels, nStages])\n","SDP_release_policy = np.empty([nStates, nLevels, nStages])\n","\n","# initialize FutureCost at 0 for all states; will update as we move backwards\n","FutureExpCost = np.zeros([nStates, nLevels])\n","\n","# begin backward-moving SDP\n","\n","\n","# print the release policy of each stage\n"],"metadata":{"id":"2Kg9khswXUf6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part d\n","\n","Simulate 50 years of operations in which the release each season is determined by the DDP policy found in part (a). Repeat this using the SDP operating policy found to be optimal using DP in part (c). In both cases, start the simulation at the target reservoir storage for the first season. If there is insufficient water to meet the release prescribed by the policy, only release as much water as is available. Likewise, if the prescribed release would result in exceeding the reservoir capacity, release as much as needed to prevent that (this may not meet the irrigation requirement, or may exceed 50, but that's okay for the purpose of this simulation)."],"metadata":{"id":"KXTMI0l4-oto"}},{"cell_type":"markdown","source":["$\\color{red}{\\text{The code below is complete.}}$"],"metadata":{"id":"Vy_nyZYxlPaE"}},{"cell_type":"code","source":["######################## Simulation ########################\n","\n","# initialize storages and releases for simulation of 50 years of 3 seasons with NLP and DP policies\n","nYears = 50\n","nSeasons = 4\n","\n","class Solution():\n","  # initialize Solution class with certain attributes for DP vs. NLP solution\n","  def __init__(self):\n","    self.simS = np.zeros([nYears,nSeasons])\n","    self.simR = np.zeros([nYears,nSeasons])\n","    self.simQwet = np.zeros([nYears,nSeasons])\n","    self.simCwet = np.zeros([nYears,nSeasons])\n","    self.S_costs = np.zeros([nYears])\n","    self.Qwet_costs = np.zeros([nYears])\n","    self.Cwet_costs = np.zeros([nYears])\n","    self.Total_costs = np.zeros([nYears])\n","    self.prescribedR = None\n","    self.Rmin_violations = 0\n","    self.Rmax_violations = 0\n","\n","  # method of Solution class to calculate simulated R and S\n","  def getSimStates(self, Q, year, season):\n","    # adjust prescribed release if not physically possible\n","    # R = min(prescribedR, simS + Q) prevents it from releasing more water than is available\n","    # max(simS + Q - K, R) prevents storage capacity from being exceeded\n","    self.simR[year,season] = max(self.simS[year,season] + Q - K,\n","                                 min(self.prescribedR, self.simS[year,season] + Q))\n","\n","    # find number of violations of maxR and Qirr\n","    if self.simR[year,season] > maxR:\n","      self.Rmax_violations += 1\n","    elif self.simR[year,season] < Qirr[season]:\n","      self.Rmin_violations += 1\n","\n","    # calculate new storage\n","    if season != (nSeasons-1): # storage in next season of same year\n","      self.simS[year,season+1] = self.simS[year,season] + Q - self.simR[year,season]\n","    elif year != (nYears-1): # storage in season 1 of next year\n","      self.simS[year+1,0] = self.simS[year,season] + Q - self.simR[year,season]\n","\n","    # calculate Qwet and Cwet from mass balance\n","    self.simQwet[year,season] = self.simR[year,season] - Qirr[season] + Qirr_RF*Qirr[season]\n","    self.simCwet[year,season] = (Cres*(self.simR[year,season] - Qirr[season]) +\n","                                     Cirr*Qirr_RF*Qirr[season]) / self.simQwet[year,season]\n","\n","  # method of Solution class to calculate cost (total deviation from targets) over simulation\n","  def getSimCost(self, year):\n","    self.S_costs[year] = np.sum(np.abs(self.simS[year,:] - S_target)*100 / S_target)\n","    for season in range(nSeasons):\n","      self.Qwet_costs[year] += max(0,Qwet_minTarget[season] - self.simQwet[year,season])*100 / Qwet_minTarget[season]\n","      self.Cwet_costs[year] += max(0,self.simCwet[year,season] - Cwet_maxTarget)*100 / Cwet_maxTarget\n","\n","    self.Total_costs[year] = self.S_costs[year] + self.Qwet_costs[year] + self.Cwet_costs[year]"],"metadata":{"id":"ceDJl2JKoWbC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["$\\color{red}{\\text{Complete the code below to run the simulation with DDP and SDP}}$"],"metadata":{"id":"nyggA0RelWGV"}},{"cell_type":"code","source":["from scipy.interpolate import RegularGridInterpolator as interp2d\n","\n","# create objects of Solution class for DDP and SDP solutions\n","DDP = Solution()\n","SDP = Solution()\n","\n","# start at target storage\n","DDP.simS[0,0] = S_target[0]\n","SDP.simS[0,0] = S_target[0]\n","\n","# vector of standard normal random variables for flow simulation\n","Z = np.zeros([nYears*nSeasons+1])\n","\n","# generate prior season's random normal inflow\n","seed = 0\n","Z[seed] = ss.norm.rvs(0,1,1)[0]\n","Qpast = np.exp(Z[seed-1]*sigmaY[-1] + muY[-1])\n","\n","# simulate operations over 50 years of 4 seasons\n"],"metadata":{"id":"_ul1xqclYFJn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part e\n","\n","Based on your simulation from part (d), make a 2x2 panel figure of the empirical cumulative distribution function of percent deviations from the storage, salinity, and wetland flow targets, as well as the sum across all three. Do this for the policies from parts (a) and (c) using a different color for each. Discuss the differences you see in performance between the operating policies found using DDP vs. SDP and why."],"metadata":{"id":"u-dNtOTKoaHw"}},{"cell_type":"markdown","source":["$\\color{red}{\\text{Use the code chunk below to make your plot.}}$"],"metadata":{"id":"8NXGh2HTlwlr"}},{"cell_type":"code","source":[],"metadata":{"id":"nlu5Gfq0odeM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part f\n","\n","Make a bar chart of release violations below Qirr and above Rmax for each solution method. How do the two methods compare?"],"metadata":{"id":"zlwoKXx_isPi"}},{"cell_type":"markdown","source":["$\\color{red}{\\text{Use the code chunk below to make your plot.}}$"],"metadata":{"id":"OjD8JfPritFa"}},{"cell_type":"code","source":[],"metadata":{"id":"jfb65V7OiuVE"},"execution_count":null,"outputs":[]}]}