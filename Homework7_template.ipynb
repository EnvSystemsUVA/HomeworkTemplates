{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1rGIQzV51fqOUaSprEZCEknKDI9aOiCml","timestamp":1711763478779}],"authorship_tag":"ABX9TyNTYrJGJ9mG8/D9pxQls87Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Homework 7"],"metadata":{"id":"DD1dlbPypMFb"}},{"cell_type":"markdown","source":["In the first class of the semester, you had to operate a reservoir for 6 seasons with the goal of having the highest end-period storage without overtopping. Here you'll formally optimize those operations! Your objective is to maximize the total storage level over the 6 seasons, subject to ensuring a $<1\\%$ chance of overtopping. The reservoir has a capacity of $K=200$ units and a maximum release of $R_{max}=150$ units. The inflows, $Q$, are log-normally distributed with the log-space means $\\mu_Y$ and standard deviations $\\sigma_Y$ listed in Table 1, where $Y=\\text{ln}(Q)$. The correlation in log-space flows between consecutive seasons is $\\rho=0.5$.\n","\n","| Parameter | Season 1 | Season 2 | Season 3 | Season 4 | Season 5 | Season 6 |  \n","|-----------|----------|----------|----------|----------|----------|----------|  \n","| $\\mu_Y$ | 2.9 | 3.9 | 4.6 | 4.6 | 4.1 | 3.7 |\n","| $\\sigma_Y$ | 0.5 | 0.4 | 0.3 | 0.4 | 0.3 | 0.2 |\n","\n","$\\color{red}{\\text{The code below initializes these parameters. It is complete.}}$"],"metadata":{"id":"qoULidpQDksj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ow68ZGDnpHkD"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from scipy import stats as ss\n","from matplotlib import pyplot as plt\n","import matplotlib\n","\n","# capacity of reservoir\n","K = 200\n","\n","# maximum release\n","Rmax = 150\n","\n","# parameters of random inflow\n","rho = 0.5 # lag-1 autocorrelation coefficient\n","muY = np.array([2.9, 3.9, 4.6, 4.6, 4.1, 3.7]) # average of log-space flows for first 6 time steps\n","sigmaY = np.array([0.5, 0.4, 0.3, 0.4, 0.3, 0.2]) # standard deviation of log-space flows for first 6 time steps\n","nSeasons = len(muY)\n","meanQ = np.exp(muY + 0.5*sigmaY**2) # do not repeat first season as last"]},{"cell_type":"markdown","source":["## Part a: DDP  \n","\n","Write a function $\\texttt{calcValueDDP}$ for discrete deterministic dynamic programming (DDP) that takes as input the following:\n","\n","* $S$: 1-D array of storages representing the states\n","* $Q$: scalar of the mean inflow for the season whose releases are being optimized,\n","* $bounds$: 1-D array of length 2 with the lower and upper bounds on the release\n","* $FutureValue$: 1-D array of the future values (total future storages) associated with the states $S$ in the following season.\n","\n","Have the function return the following:\n","\n","* $Rbest$: 1-D array of optimal releases from each state in $S$\n","* $Vbest$: 1-D array of the present + future value associated with each state in $S$\n","\n","$\\color{red}{\\text{Complete the code below.}}$"],"metadata":{"id":"N48C7SA6GaBz"}},{"cell_type":"code","source":["def calcValueDDP(S, Q, bounds, FutureValue):\n","\n","\n","\n","  return Rbest, Vbest"],"metadata":{"id":"J_9WkUkV2tqr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now use the following code chunk to perform discrete DDP with your $\\texttt{calcValueDDP}$ function, solving for the optimal release each season from storage levels between 0 and 200 in increments of 20 assuming the mean inflow each season. $\\color{red}{\\text{The code below is complete.}}$"],"metadata":{"id":"fELjwltMH_Dj"}},{"cell_type":"code","source":["# get indices of stages\n","nStages = len(meanQ)\n","forward_indices = np.arange(nStages)\n","backward_indices = forward_indices[::-1]\n","backward_indices = np.insert(backward_indices,0,0)\n","\n","# discretize states\n","states = np.arange(0,201,20)\n","nStates = len(states)\n","\n","# bounds on decision variables (releases)\n","# R is between 0 and R max each season\n","bounds = [0,Rmax]\n","\n","# initialize matrices with costs of each state at each stage\n","# and optimal releases to make from each state at each stage\n","DDP_values = np.empty([nStates,nStages])\n","DDP_release_policy = np.empty([nStates,nStages])\n","\n","# initialize FutureValue at 0 for all states; will update as we move backwards\n","DDP_FutureValue = np.zeros([nStates])\n","\n","# begin backward-moving DP\n","loop = True\n","while loop:\n","  count = 0\n","  for index in backward_indices[0:-1]:\n","    # find optimal release and value of each state in this stage\n","    R, DDP_FutureValue = calcValueDDP(states, meanQ[index-1], bounds, DDP_FutureValue)\n","\n","    # count iterations with no change in optimal release\n","    if np.all(R == DDP_release_policy[:,index-1]):\n","      count += 1\n","\n","    # update best releases and value of each state if not yet in steady state\n","    DDP_values[:,index] = DDP_FutureValue\n","    DDP_release_policy[:,index-1] = R\n","\n","  # stop loop if no change in optimal decisions across all iterations\n","  if count == len(backward_indices[0:-1]):\n","    break\n","\n","DDP_release_policy_df = pd.DataFrame(DDP_release_policy,\n","                                 columns=[\"Season 1\",\"Season 2\",\"Season 3\",\"Season 4\",\"Season 5\", \"Season 6\"],\n","                                 index=states)\n","DDP_release_policy_df.index.rename(\"Storage\",inplace=True)\n","DDP_release_policy_df"],"metadata":{"id":"w07y5qqrIAok"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["$\\color{red}{\\text{Now copy the code above, but instead of assuming the mean inflow each season, pass the 99th percentile flow each season to $\\texttt{calcValueDDP}$.}}$\n","$\\color{red}{\\text{Call your new variables DDP_values_99, DDP_release_policy_99, DDP_FutureValue_99 and DDP_release_policy_99_df.}}$  \n","$\\color{red}{\\text{Use the tolerance approach we used for SDP in class as a stopping criterion.}}$$"],"metadata":{"id":"2C8qhCHfa8VW"}},{"cell_type":"code","source":["# find the 99th percentile flow each season\n","q_99 =\n","\n","# re-run DDP using q_99 each season instead of meanQ\n"],"metadata":{"id":"t0HlFTiJbJ1r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## b. Part b: SDP\n","\n","Now find an optimal release policy using SDP. First, use the code below to discretize the inflows each season into 20 values between that season's 1st and 99th percentile, and then calculate the transition probabilities between them. $\\color{red}{\\text{The code below is complete.}}$"],"metadata":{"id":"G-NPqg5FI9Fc"}},{"cell_type":"code","source":["forward_indices = np.append(forward_indices,0)\n","\n","# find transition probabilities from discrete flow values in one season to the next\n","def calcTransProb(mu, sigma, rho, nLevels):\n","  transprob = np.empty([nLevels,nLevels])\n","  # discrete levels of log-space flows\n","  Ylevels1 = np.linspace(ss.norm.ppf(0.01,mu[0],sigma[0]),ss.norm.ppf(0.99,mu[0],sigma[0]),nLevels)\n","  Ylevels2 = np.linspace(ss.norm.ppf(0.01,mu[1],sigma[1]),ss.norm.ppf(0.99,mu[1],sigma[1]),nLevels)\n","  for i in range(nLevels):\n","    # find conditional distribution of Qlevels2[j] given flow is Qlevels1[i] in previous season\n","    mu_cond = mu[1] + rho*(sigma[0]/sigma[1])*(Ylevels1[i] - mu[0])\n","    sigma_cond = sigma[1] * np.sqrt((1-rho**2))\n","    for j in range(nLevels):\n","      transprob[i,j] = ss.norm.pdf(Ylevels2[j], mu_cond, sigma_cond)\n","\n","    #normalize probabilities to sum to 1\n","    transprob[i,:] = transprob[i,:] / np.sum(transprob[i,:])\n","\n","  return transprob\n","\n","# discretize inflows into nLevels and find transition probabilities between them\n","nLevels=20\n","transprob = []\n","for i, index in enumerate(forward_indices[0:-1]):\n","  mu = np.array([muY[index],muY[forward_indices[i+1]]]) # log-space mean\n","  sigma = np.array([sigmaY[index],sigmaY[forward_indices[i+1]]]) # log-space standard deviation\n","  transprob.append(calcTransProb(mu, sigma, rho, nLevels))"],"metadata":{"id":"CRM8y_Vs4fTZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now modify the two functions $\\texttt{calcValueSDP}$ and $\\texttt{findBestR}$ from class to optimize operations for this problem with SDP.  \n","\n","$\\texttt{calcValueSDP}$ will loop through all discretized storage levels at time $t$ and inflow levels at time $t-1$ to determine the optimal release conditioned on those two values that maximizes the present + expected future value (average storage). Within each of these nested loops, it will do that by calling $\\texttt{findBestR}$ with $\\texttt{scipy.optimize.minimize}$. Note, we would like to maximize, not minimize, the value, though. Constraint this optimization so that the reservoir capacity is not exceeded even if the 99th percentile flow is received at stage $t$, and the storage is not negative even if the 1st percentile flow is received at stage $t$\n","\n","$\\texttt{calcValueSDP}$ should take as input the following:\n","\n","* $S$: 1-D array of discrete storage values representing the states\n","* $Ylevels1$: 1-D array of discrete, log-space inflow levels in the previous stage, that current releases will be conditioned on for each storage level\n","* $Ylevels2$: 1-D array of discrete, log-space inflow levels in this stage\n","* $transprob$: 2-D array of transition probabilities from $Ylevels1$ to $Ylevels2$\n","* $bounds$: 1-D array of length 2 with the lower and upper bounds on the release\n","* $FutureExpCost$: 2-D array of future expected costs at each combination of $S$ and $Ylevels1$ that will be added to the cost of the optimal state transition at this stage to compute the present + expected future value\n","\n","And output:\n","\n","* $Rbest$: 2-D array of optimal releases as a function of $S$ and $Ylevels1$\n","* $Cbest$: 2-D array of present + expected future values associated with each combination of $S$ and $Ylevels1$\n","\n","$\\texttt{findBestR}$ should take as input the following:\n","\n","* $R$: scalar representing the release we are trying to find the optimal value for\n","* $s$: scalar representing the storage level at which we are trying to find the optimal release\n","* $y1$: scalar representing the inflow level in the previous time step we are conditioning the release on\n","* $j$: scalar index representing which of the previous time step's inflow levels we are conditioning on\n","* $S$: 2-D array of storage states for interpolating the future value\n","* $Ylevels2$: 1-D array of discrete, log-space inflow levels in this stage\n","* $transprob$: 2-D array of transition probabilities from $Ylevels1$ to $Ylevels2$\n","* $FutureExpValue$: 2-D array of future expected costs at each combination of $S$ and $Ylevels1$ that will be added to the cost of the optimal state transition at this stage to compute the present + expected future value\n","\n","And output:\n","\n","* $-V$: the expected future value associated with the release $R$ at this storage $s$ and inflow $y1$, negated for minimization\n","\n","$\\color{red}{\\text{Complete these functions in the code chunk below.}}$"],"metadata":{"id":"n9NtTrBFJaiw"}},{"cell_type":"code","source":["######################## SDP Optimization ########################\n","from scipy.optimize import minimize\n","\n","def findBestR(R, s, y1, j, S, Ylevels2, transprob, FutureExpValue):\n","\n","\n","  return -V\n","\n","def calcValueSDP(S, Ylevels1, Ylevels2, Qguess, transprob, bounds, FutureExpValue):\n","\n","\n","  return Rbest, Vbest"],"metadata":{"id":"39vfewgH4lHW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now run backward-moving SDP with the code chunk below, which uses the functions you wrote above to solve for the optimal release from each combination of 20 storage levels and 20 previous inflow levels. It stops when the average difference in the release decisions at each state change by less than $0.1\\%$ in a given stage from one cycle through all seasons to the next. $\\color{red}{\\text{The code below is complete.}}$"],"metadata":{"id":"mkmwirMEOyJT"}},{"cell_type":"code","source":["# use same stages, states, indices and bounds as for DDP\n","# initialize matrices with values of each state at each stage\n","# and optimal releases to make from each state at each stage\n","SDP_values = np.empty([nStates, nLevels, nStages])\n","SDP_release_policy = np.empty([nStates, nLevels, nStages])+Rmax\n","\n","# initialize FutureValue at 0 for all states; will update as we move backwards\n","SDP_FutureExpValue = np.zeros([nStates, nLevels])\n","\n","# begin backward-moving SDP\n","tolerance = 0.1# average % difference in optimal releases from one cycle to the next at which to stop looping\n","avgPctDiff = np.inf\n","cycle = 0 # number of times cycling through all seasons\n","while loop:\n","  count = 0\n","  cycle += 1\n","  print(cycle)\n","  for index in backward_indices[0:-1]:\n","    mu = np.array([muY[index-2],muY[index-1]])\n","    sigma = np.array([sigmaY[index-2],sigmaY[index-1]])\n","    Ylevels1 = np.linspace(ss.norm.ppf(0.01,mu[0],sigma[0]),ss.norm.ppf(0.99,mu[0],sigma[0]),nLevels)\n","    Ylevels2 = np.linspace(ss.norm.ppf(0.01,mu[1],sigma[1]),ss.norm.ppf(0.99,mu[1],sigma[1]),nLevels)\n","\n","    # find optimal release and value of each state in this stage\n","    # states (storage targets) are for next period (index) while inflows and releases are for this period (index-1)\n","    R, FutureExpValue = calcValueSDP(states, Ylevels1, Ylevels2, meanQ[index-1], transprob[index-1], bounds, SDP_FutureExpValue)\n","\n","    # update best releases and value of each state\n","    SDP_values[:,:,index] = SDP_FutureExpValue\n","    SDP_release_policy[:,:,index-1] = R\n","\n","    # find average % difference in optimal releases at this stage compared to the last loop\n","    if cycle > 1:\n","      avgPctDiff = np.nanmean(np.abs(R - SDP_release_policy[:,:,index-1])*100/SDP_release_policy[:,:,index-1])\n","      if avgPctDiff < tolerance:\n","        count += 1\n","\n","  # stop loop if average % change in optimal decisions across all iterations < tolerance\n","  if count == len(backward_indices[0:-1]):\n","    break\n","\n","# print the release policy of each stage\n","for i in range(nSeasons):\n","  cols = []\n","  Ylevels = np.linspace(ss.norm.ppf(0.01,muY[i-1],sigmaY[i-1]),ss.norm.ppf(0.99,muY[i-1],sigmaY[i-1]),nLevels)\n","  for j in range(nLevels):\n","    cols.append(\"Q = %0.2f\" % np.exp(Ylevels[j]))\n","  SDP_release_policy_df = pd.DataFrame(SDP_release_policy[:,:,i], columns=cols,index=states)\n","  SDP_release_policy_df.index.rename(\"Storage\",inplace=True)\n","  print(\"Season \" + str(i+1) + \" release policy\", SDP_release_policy_df)"],"metadata":{"id":"ahkMrbqh6i7A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part c: Simulation\n","\n","Now simulate operations with your DDP and SDP policies for 50 years of 6 seasons. If the storage capacity is going to be exceeded, the code below will increase the release to that which prevents it from being exceeded. This may violate the max release constraint. The number of these Rmax_violations is also calculated.\n","\n","$\\color{red}{\\text{Edit getSimCost below to return the average simulated storage level of the operations each year.}}$"],"metadata":{"id":"DvCq9hv-PrF5"}},{"cell_type":"code","source":["######################## Simulation ########################\n","\n","# initialize storages and releases for simulation of 50 years of 3 seasons with NLP and DP policies\n","nYears = 50\n","nSeasons = 6\n","\n","class Solution():\n","  # initialize Solution class with certain attributes for DP vs. NLP solution\n","  def __init__(self):\n","    self.simS = np.zeros([nYears,nSeasons])\n","    self.simR = np.zeros([nYears,nSeasons])\n","    self.value = np.zeros([nYears])\n","    self.prescribedR = None\n","    self.Rmax_violations = 0\n","\n","  # method of Solution class to calculate simulated R and S\n","  def getSimStates(self, Q, year, season):\n","    # adjust prescribed release if not physically possible\n","    # R = min(prescribedR, simS + Q) prevents it from releasing more water than is available\n","    # max(simS + Q - K, R) prevents storage capacity from being exceeded\n","    self.simR[year,season] = max(self.simS[year,season] + Q - K,\n","                                 min(self.prescribedR, self.simS[year,season] + Q))\n","\n","    # find number of violations of maxR and Qirr\n","    if self.simR[year,season] > Rmax:\n","      self.Rmax_violations += 1\n","\n","    # calculate new storage\n","    if season != (nSeasons-1): # storage in next season of same year\n","      self.simS[year,season+1] = self.simS[year,season] + Q - self.simR[year,season]\n","    elif year != (nYears-1): # storage in season 1 of next year\n","      self.simS[year+1,0] = self.simS[year,season] + Q - self.simR[year,season]\n","\n","  # method of Solution class to calculate value (average storage) over simulation\n","  def getSimCost(self, year):\n"],"metadata":{"id":"vqFqhMQuXY5s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now run the code below to compute the value of DDP_avg, DDP_99 and SDP over the 50-year simulation, and compute its average value and number of Rmax violations. Each policy is initiated at a storage of 100 (half-full). $\\color{red}{\\text{The code below is complete.}}$"],"metadata":{"id":"nphFGNXHQ2s8"}},{"cell_type":"code","source":["from scipy.interpolate import RegularGridInterpolator as interp2d\n","\n","# create objects of Solution class for NLP and DP solutions\n","DDP_avg = Solution()\n","DDP_99 = Solution()\n","SDP = Solution()\n","\n","# start half full\n","DDP_avg.simS[0,0] = 100\n","DDP_99.simS[0,0] = 100\n","SDP.simS[0,0] = 100\n","\n","# vector of standard normal random variables for flow simulation\n","Z = np.zeros([nYears*nSeasons+1])\n","\n","# generate prior season's random normal inflow\n","seed = 0\n","np.random.seed(seed)\n","Z[seed] = ss.norm.rvs(0,1,1)[0]\n","Qpast = np.exp(Z[seed-1]*sigmaY[-1] + muY[-1])\n","\n","# simulate operations over 50 years of 4 seasons\n","for year in range(nYears):\n","  for season in range(nSeasons):\n","    # generate inflow (set a seed to make it reproducible)\n","    seed += 1\n","    np.random.seed(seed)\n","    # generate flow conditional on previous season's flow and transform to real-space\n","    Z[seed] = rho*(Z[seed-1]) + ss.norm.rvs(0,1,1)[0]*np.sqrt(1-rho**2)\n","    Qnow = np.exp(Z[seed]*sigmaY[season] + muY[season])\n","\n","    # find DDP releases from its policy: interpolate release between nearest storages\n","    DDP_avg.prescribedR = np.interp(DDP.simS[year,season],states,DDP_release_policy[:,season])\n","    DDP_99.prescribedR = np.interp(DDP_99.simS[year,season],states,DDP_release_policy_99[:,season])\n","\n","    # find SDP releases from its policy: interpolate release between nearest storages and flows\n","    Ylevels1 = np.linspace(ss.norm.ppf(0.01,muY[season-1],sigmaY[season-1]),ss.norm.ppf(0.99,muY[season-1],sigmaY[season-1]),nLevels)\n","    f = interp2d((states, Ylevels1), SDP_release_policy[:,:,season])\n","    if np.log(Qpast) <= Ylevels1[0]: # interpolate between storages at lowest flow level\n","      SDP.prescribedR = np.interp(SDP.simS[year,season],states,SDP_release_policy[:,0,season])\n","    elif np.log(Qpast) >= Ylevels1[-1]: # interpolate between storages at highest flow level\n","      SDP.prescribedR = np.interp(SDP.simS[year,season],states,SDP_release_policy[:,-1,season])\n","    else: # interpolate over 2-D grid\n","      SDP.prescribedR = f(np.array([SDP.simS[year,season],np.log(Qpast)]))[0]\n","\n","    # find actual release (what is physically possible) and calculate storage from mass balance\n","    DDP_avg.getSimStates(Qnow, year, season)\n","    DDP_99.getSimStates(Qnow, year, season)\n","    SDP.getSimStates(Qnow, year, season)\n","\n","    # update past flow for next season's calculation\n","    Qpast = Qnow\n","\n","  # calculate total cost (squared deviations from targets) in simulated year\n","  DDP_avg.getSimCost(year)\n","  DDP_99.getSimCost(year)\n","  SDP.getSimCost(year)"],"metadata":{"id":"e2ZFAZ-GXuLz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## d. Plot simulation results: CDF of value\n","\n","Use the code chunk below to make a CDF of the average value from DDP_avg, DDP_99 and SDP across the 50-year simulation. What do you observe?"],"metadata":{"id":"4xjMMaFiQAxY"}},{"cell_type":"code","source":[],"metadata":{"id":"n9NkK5UdY2H6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## e. Plot simulation results: Rmax violations\n","\n","Now make a bar plot of the number of release violations from DDP_avg, DDP_99, and SDP over the 50-years. What do you notice? Based on the results in parts (d) and (e), which method would you recommend using to optimize reservoir operations and why?"],"metadata":{"id":"wPCie59tQDnd"}},{"cell_type":"code","source":[],"metadata":{"id":"XlPmgwCEZJ2B"},"execution_count":null,"outputs":[]}]}